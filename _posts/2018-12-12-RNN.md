---
math: true
category: Knowlege
path: '/Knowlege/:id'
title: 'RNN'
type: 'TIPS'

layout: nil
---

### RNN

The idea of an RNN is a network with fixed input and output, which is being applied to the sequence of objects and can pass information along this sequence. This information is called hidden state and is normally just a vector of numbers of some size.

#### **Unfold RNN (unfold by time)** 

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/800px-Recurrent_neural_network_unfold.svg.png" width="70%">

RNN produce different output for the same input in different contexts, RNNs can be seen as a standard building block of the systems that need to process variable-length input.

####  **long short-term memory networks (LSTM)**

LSTM first proposed in 1997. It performs good in speech recognition and text-to-speech synthesis.

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/800px-Long_Short-Term_Memory.svg.png" width="70%">

####  gated recurrent units(GRU)

GRU proposed in 2014, Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM, as they lack an output gate.

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Gated_Recurrent_Unit.svg/1920px-Gated_Recurrent_Unit.svg.png" width="70%">



#### encoder-decoder

use an RNN to process an input sequence and encode this sequence into some fixed-length representation. This RNN is called an encoder. Then you feed the encoded vector into another RNN, called a decoder, which has to produce the resulting sequence. It is widely used in machine translation.

| <img src="https://pic2.zhimg.com/v2-278b5920ac2b4fc8c2319c90eaa7f9db_1200x500.jpg" width="70%"> |
| :----------------------------------------------------------: |
|                   curriculum learning mode                   |

#### attention mechanism

| <img src="https://3.bp.blogspot.com/-3Pbj_dvt0Vo/V-qe-Nl6P5I/AAAAAAAABQc/z0_6WtVWtvARtMk0i9_AtLeyyGyV6AI4wCLcB/s1600/nmt-model-fast.gif" width="70%"> |
| :----------------------------------------------------------: |
|                seq2seq (picture from Google)                 |

