---
category: Knowlege
path: '/Knowlege/:id'
title: 'Reinforcement learning'
type: 'TIPS'

layout: nil
---
## RL note

### classification

- model-based: previous observation **predict** following rewards and observations
- model-free: train it by intuition 
- police-based: **directly** approximating the policy of the agent
- value-based: the agent calculates the **value of every possible action**
- off police: the ability of the method to learn on old **historical data** (obtained
- on police: requires **fresh data** obtained from the environment





## Police-based method

**just like a classification problem**

- NN input: observation
- NN output: distribution of actions
- agent: random choose action base on distribution of actions(police)



#### Cross-entropy method

1. Play N number of episodes using our current model and environment. 
2. Calculate the total reward for every episode and decide on a reward boundary. Usually, we use some percentile of all rewards, such as 50th or 70th.
3. Throw away all episodes with a reward below the boundary. 
4. Train on the remaining "elite" episodes using observations as the input and issued actions as the desired output.
5. Repeat from step 1 until we become satisfied with the result.

use **cross-entropy loss** function as loss function

**drawback:** Cross-entropy methods have difficult to understand which step or which state is good and which is not good, it just know overall this episode is better or not



## Tabular Learning

### Why using Q but not V?

​	if I know the value of current state, I know the state is good or not, but I don't know how to choose next action, even I know the V of all next state, I **can not directly** know which action i need to do, so we decide action base on Q.

​	if I know Q of all available action, we just choose the action which has max Q, then this action surely has max V according the definition of V(the relationship of Q and V).

### The value iteration in the Env with a loop

If there is no gamma(gamma = 1) and the environment has a loop, the value of state will be infinite.

### problems  in Q-learning

- state is not discrete
- state space is is very large
-  don't know probability of action and reward matrix(P(s',r|s,a)). 

### Value iteration 

#### Reward table

- index: "source state" + "action" + "target state"
- value: reward

#### Transition table

- index: "state" + "action"
- value:  index: state  value: counts

#### Value table

- index: state
- value:  value of state

#### Steps

1. random action to build reward and transitions table

2. perform a value iteration loop over all state

3. play several full episodes to choose the best action using the updated value table, at the same time, update reward and transitions table using new data.

**Problems of separating training and testing **: When using the previous steps, you actually separate training and testing, it may has another problem, since the task may be difficult,using random action is hard to reach the final state, so you may lack some states which are near the final step. So, maybe you should conduct training and testing at the same time, and add some exploit into testing.

### Q-learning

Different to value iteration,Q-learn change the value table to Q value table:

#### Q value table

- index:  "state" + "action"
- value:  action value(Q)

Here `state_value(state) = action_values[(state, best_action)]`

## DQN


































## NN

#### sigmoid

*<u>It transfer a value input to (0,1)</u>*


$$
f(x)=\frac{L}{1+e^{-x}} = \frac{e^{x}}{e{x}+1}
$$

#### **softmax**

In short, *<u>It transfer K-dimensional vector input to (0,1)</u>*

In mathematics, the softmax function, or normalized exponential function, is a generalization of the logistic function that "squashes" a K-dimensional vector **z**  of arbitrary real values to a K-dimensional vector  \sigma(**z**) of real values, where each entry is in the range (0, 1), and all the entries add up to 1.

#### tanh

*<u>It transfer a value input to (-1,1)</u>*


$$
f(x)=tanh(x)= \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
$$

#### **relu**

$$
f(x)=max(0,x)
$$

![1541474023324](/home/dd/.config/Typora/typora-user-images/1541474023324.png)